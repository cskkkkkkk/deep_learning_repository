{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba3dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--exp_name EXP_NAME]\n",
      "                             [--logger_filename LOGGER_FILENAME]\n",
      "                             [--dump_path DUMP_PATH] [--exp_id EXP_ID]\n",
      "                             [--seed SEED] [--model_name MODEL_NAME]\n",
      "                             [--is_load_ckpt_if_exists] [--ckpt CKPT]\n",
      "                             [--dropout DROPOUT] [--hidden_dim HIDDEN_DIM]\n",
      "                             [--data_path DATA_PATH]\n",
      "                             [--entity_list ENTITY_LIST]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR] [--mu MU]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--info_per_epochs INFO_PER_EPOCHS]\n",
      "                             [--save_per_epochs SAVE_PER_EPOCHS]\n",
      "                             [--training_epochs TRAINING_EPOCHS]\n",
      "                             [--schedule SCHEDULE] [--gamma GAMMA]\n",
      "                             [--early_stop EARLY_STOP]\n",
      "                             [--evaluate_interval EVALUATE_INTERVAL]\n",
      "                             [--cfg CFG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/cs/.local/share/jupyter/runtime/kernel-02f505b2-a034-457d-a531-198fc8dd6dd2.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs/anaconda3/envs/pytorch/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "from src.utils import *\n",
    "from src.dataloader import *\n",
    "from src.trainer import *\n",
    "from src.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dfba7",
   "metadata": {},
   "source": [
    "main.py文件中要补全的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTagger(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, model_name):\n",
    "        super(BertTagger, self).__init__()\n",
    "        # TODO:\n",
    "        # （1）利用AutoConfig.from_pretrained定义config\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        # （2）利用AutoModelWithLMHead.from_pretrained定义模型，注意要传入刚才的config\n",
    "        self.bert_model = AutoModelWithLMHead.from_pretrained(model_name, config=config)\n",
    "        # （3）定义一个线性层用于分类预测\n",
    "        self.classifier = nn.Linear(config.hidden_size, output_dim)\n",
    "        # 提示：参考文档https://huggingface.co/bert-base-cased\n",
    "\n",
    "    def forward(self, X):\n",
    "        # TODO: \n",
    "        # （1）把X输入bert_model得到hidden_states；\n",
    "        outputs = self.bert_model(X, attention_mask=torch.ones(X.shape).to(X.device), output_hidden_states=True)\n",
    "        # （2）提取其中属于最后一个transformer layer的hidden_states作为最终特征；\n",
    "        # （3）最后把特征输入线性层完成预测。\n",
    "        # 提示：需要用到output_hidden_states参数，并参考以下文档\n",
    "        # https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/bert#transformers.BertLMHeadModel\n",
    "        features =  outputs.hidden_states[-1]\n",
    "        logits = logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params):\n",
    "    if params.seed:\n",
    "        random.seed(params.seed)\n",
    "        np.random.seed(params.seed)\n",
    "        torch.manual_seed(params.seed)\n",
    "        torch.cuda.manual_seed(params.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    logger = init_experiment(params, logger_filename=params.logger_filename)\n",
    "    logger.info(params.__dict__)\n",
    "    domain_name = os.path.basename(params.data_path[0])\n",
    "    if domain_name == '':\n",
    "        domain_name = os.path.basename(params.data_path[0][:-1])\n",
    "    ner_dataloader = NER_dataloader(data_path=params.data_path,\n",
    "                                    domain_name=domain_name,\n",
    "                                    batch_size=params.batch_size, \n",
    "                                    entity_list=params.entity_list)\n",
    "    dataloader_train, dataloader_dev, dataloader_test = ner_dataloader.get_dataloader()\n",
    "    label_list = ner_dataloader.label_list\n",
    "    entity_list = ner_dataloader.entity_list\n",
    "\n",
    "    if params.model_name in ['bert-base-cased', 'roberta-base']:\n",
    "        model = BertTagger(hidden_dim=params.hidden_dim,\n",
    "                            output_dim=len(label_list), \n",
    "                            model_name=params.model_name)\n",
    "    else:\n",
    "        raise Exception('model name %s is invalid' % params.model_name)\n",
    "    model.cuda()\n",
    "    trainer = BaseTrainer(params, model, entity_list, label_list)\n",
    "\n",
    "    logger.info(\"Training ...\")\n",
    "    no_improvement_num = 0\n",
    "    best_f1 = 0\n",
    "    step = 0\n",
    "    loss_history = []\n",
    "    f1_history = []\n",
    "\n",
    "    logger.info(\"Initial lr is %s\" % (str(trainer.scheduler.get_last_lr())))\n",
    "\n",
    "    for e in range(1, params.training_epochs+1):\n",
    "        logger.info(\"============== epoch %d ==============\" % e)\n",
    "        loss_list = []\n",
    "        mean_loss = 0.0\n",
    "        total_cnt = 0\n",
    "        correct_cnt = 0\n",
    "\n",
    "        pbar = tqdm(dataloader_train, total=len(dataloader_train))\n",
    "        for X, y in pbar:\n",
    "            step += 1\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            trainer.batch_forward(X)\n",
    "            correct_cnt += int(torch.sum(torch.eq(torch.argmax(trainer.logits, dim=2), y).float()).item())\n",
    "            total_cnt += trainer.logits.size(0) * trainer.logits.size(1)\n",
    "            trainer.batch_loss(y)\n",
    "            loss = trainer.batch_backward()\n",
    "            loss_list.append(loss)\n",
    "            mean_loss = np.mean(loss_list)\n",
    "            pbar.set_description(\"Epoch %d, Step %d: Loss=%.4f, Training_acc=%.2f%%\" % (\n",
    "                e, step, mean_loss, correct_cnt / total_cnt * 100\n",
    "            ))\n",
    "        loss_history.append(mean_loss)\n",
    "        if params.info_per_epochs > 0 and e % params.info_per_epochs == 0:\n",
    "            logger.info(\"Epoch %d, Step %d: Loss=%.4f, Training_acc=%.2f%%\" % (\n",
    "                e, step, mean_loss, correct_cnt / total_cnt * 100\n",
    "            ))\n",
    "        if trainer.scheduler != None:\n",
    "            old_lr = trainer.scheduler.get_last_lr()\n",
    "            trainer.scheduler.step()\n",
    "            new_lr = trainer.scheduler.get_last_lr()\n",
    "            if old_lr != new_lr:\n",
    "                logger.info(\"Epoch %d, Step %d: lr is %s\" % (\n",
    "                    e, step, str(new_lr)\n",
    "                ))\n",
    "        if params.save_per_epochs != 0 and e % params.save_per_epochs == 0:\n",
    "            trainer.save_model(\"best_finetune_domain_%s_epoch_%d.pth\" % (domain_name, e), path=params.dump_path)\n",
    "        if e % params.evaluate_interval == 0:\n",
    "            f1_dev, f1_dev_each_class = trainer.evaluate(dataloader_dev, each_class=True)\n",
    "            logger.info(\"Epoch %d, Step %d: Dev_f1=%.4f, Dev_f1_each_class=%s\" % (\n",
    "                e, step, f1_dev, str(f1_dev_each_class)\n",
    "            ))\n",
    "            f1_history.append(f1_dev)\n",
    "            if f1_dev > best_f1:\n",
    "                logger.info(\"Find better model!!\")\n",
    "                best_f1 = f1_dev\n",
    "                no_improvement_num = 0\n",
    "                trainer.save_model(\"best_finetune_domain_%s.pth\" % domain_name, path=params.dump_path)\n",
    "            else:\n",
    "                no_improvement_num += 1\n",
    "                logger.info(\"No better model is found (%d/%d)\" % (no_improvement_num, params.early_stop))\n",
    "            if no_improvement_num >= params.early_stop:\n",
    "                logger.info(\"Stop training because no better model is found!!!\")\n",
    "                break\n",
    "    logger.info(\"Finish training ...\")\n",
    "\n",
    "    logger.info(\"Testing...\")\n",
    "    trainer.load_model(\"best_finetune_domain_%s.pth\" % domain_name, path=params.dump_path)\n",
    "    trainer.model.cuda()\n",
    "    f1_test, f1_score_dict = trainer.evaluate(dataloader_test, each_class=True)\n",
    "    logger.info(\"Final Result: Evaluate on Test Set. F1: %.4f.\" % (f1_test))\n",
    "    f1_score_dict = sorted(f1_score_dict.items(), key=lambda x: x[0])\n",
    "    logger.info(\"F1_list: %s\" % (f1_score_dict))\n",
    "    logger.info(\"Finish testing ...\")\n",
    "\n",
    "    # Visualize the training process\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(loss_history, label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(f1_history, label='Dev F1 Score')\n",
    "    plt.title('Dev F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(params.dump_path, 'training_process.png'))\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    params = get_params()\n",
    "    main(params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
